
│  config.py
# -*- coding: utf-8 -*-
"""
AlphaZero 7x7 Gomoku 专用配置文件
优化参数说明：
- 棋盘尺寸: 7x7
- 胜利条件: 5子连珠
- 训练参数针对7x7棋盘优化
"""

import torch
import time
import os

class Config:
    def __init__(self):
        # 硬件配置
        self.use_gpu = torch.cuda.is_available()
        
        # 游戏规则配置
        self.board_width = 7      # 棋盘宽度
        self.board_height = 7     # 棋盘高度
        self.n_in_row = 5         # 五子连珠获胜
        
        # 神经网络训练
        self.lr = 5e-4            # 学习率 (7x7棋盘比15x15需要更大的学习率)
        self.l2_const = 1e-4      # L2正则化系数
        self.kl_targ = 0.02       # KL散度目标值
        
        # 训练流程控制
        self.batch_size = 512      # 训练批次大小
        self.epochs = 5           # 每次更新的训练轮数
        self.play_batch_size = 2   # 并行自我对弈局数
        self.buffer_size = 20000   # 经验池大小
        self.check_freq = 50       # 每50批次评估一次
        self.game_batch_num = 2000 # 总训练批次数
        
        # MCTS参数 (针对7x7优化)
        self.c_puct = 5           # 探索系数
        self.n_playout = 600       # 每步模拟次数
        self.temp = 1.0           # 温度参数
        
        # 评估参数
        self.pure_mcts_playout_num = int(self.n_playout * 1.5)
        self.eval_games = 30       # 每次评估的对局数
        self.minimax_depth = 3     # Minimax搜索深度
        
        # 动态调整参数
        self.dynamic_eval = True   # 启用智能调整
        self.max_depth_increase = 2 # Minimax最大加深层数
        
        # 日志和模型保存
        self._ensure_dirs()  # 确保所有目录存在
        timestamp = time.strftime("%Y%m%d_%H%M%S", time.localtime())
        self.log_file = f"./logs/7x5_train_log_{timestamp}.txt"
        self.model_save_freq = 100 # 每100批次保存一次模型
        self.model_dir = "./models"  # 新增：模型保存目录
        self.best_model_path = f"{self.model_dir}/best_policy.model"  # 最佳模型路径
        
        self.use_tensorboard = True
        self.tensorboard_log_dir = os.path.abspath("./logs/tensorboard")

    def _ensure_dirs(self):
        """确保所有需要的目录都存在"""
        dirs = ["./logs", "./models", "./logs/tensorboard"]
        for dir_path in dirs:
            if not os.path.exists(dir_path):
                os.makedirs(dir_path)

    def log_config(self, logger):
        """打印所有配置参数"""
        logger.info("7x7 Gomoku 训练配置:")
        logger.info(f"硬件: {'GPU' if self.use_gpu else 'CPU'}")
        logger.info(f"棋盘: {self.board_width}x{self.board_height} (连{self.n_in_row}子)")
        logger.info(f"训练参数: lr={self.lr}, batch={self.batch_size}, buffer={self.buffer_size}")
        logger.info(f"MCTS参数: playout={self.n_playout}, c_puct={self.c_puct}")
        logger.info(f"评估: 每{self.check_freq}批评估{self.eval_games}局")
        logger.info(f"模型保存路径: {os.path.abspath(self.model_dir)}")

# 全局配置实例
config = Config()


│  evaluation_metrics.py
# -*- coding: utf-8 -*-
"""
评估指标计算模块
"""

import numpy as np
from collections import defaultdict

class EvaluationMetrics:
    def __init__(self):
        self.reset()
        
    def reset(self):
        self.win_counts = defaultdict(int)
        self.move_data = []
        self.advantage_history = []
        self.key_moments = []
        
    def record_game(self, winner, moves, advantage_changes):
        """
        记录单局游戏数据
        :param winner: 获胜方 (1, 2, -1)
        :param moves: 所有移动记录
        :param advantage_changes: 每一步的优势变化
        """
        self.win_counts[winner] += 1
        self.move_data.append(len(moves))
        
        # 计算关键转折点
        turning_points = 0
        max_advantage = 0
        max_blunder = 0
        for i in range(1, len(advantage_changes)):
            delta = abs(advantage_changes[i] - advantage_changes[i-1])
            if delta > 0.3:  # 优势变化超过30%
                turning_points += 1
                self.key_moments.append({
                    'move': i,
                    'delta': delta,
                    'before': advantage_changes[i-1],
                    'after': advantage_changes[i]
                })
            if delta > max_blunder:
                max_blunder = delta
                
        self.advantage_history.append({
            'avg': np.mean(advantage_changes),
            'std': np.std(advantage_changes),
            'max_blunder': max_blunder,
            'turning_points': turning_points
        })
    
    def get_metrics(self):
        """
        返回汇总指标
        """
        win_ratio = (self.win_counts[1] + 0.5 * self.win_counts[-1]) / sum(self.win_counts.values())
        
        return {
            'win_ratio': win_ratio,
            'avg_moves': np.mean(self.move_data) if self.move_data else 0,
            'advantage_std': np.mean([x['std'] for x in self.advantage_history]),
            'max_blunder': np.max([x['max_blunder'] for x in self.advantage_history]),
            'turning_points': np.mean([x['turning_points'] for x in self.advantage_history]),
            'matchup_stats': dict(self.win_counts)
        }



│  game.py
# -*- coding: utf-8 -*-
"""
五子棋游戏规则与状态管理
中文注释版
"""

import numpy as np
import os
from colorama import init, Fore, Style

init(autoreset=True)  # 自动重置颜色

class Board:
    def __init__(self, width=8, height=8, n_in_row=5):
        self.width = width
        self.height = height
        self.n_in_row = n_in_row
        self.states = {}        # key:move(int), value:player(int)
        self.current_player = 1  # 当前玩家ID (1或2)
        self.availables = None  # 可用位置列表
        self.last_move = -1     # 最后一步

    def init_board(self, start_player=1):
        """初始化棋盘
        Args:
            start_player: 1表示玩家1先手，2表示玩家2先手
        """
        if start_player not in (1, 2):
            raise ValueError("start_player必须是1或2")
        
        self.current_player = start_player
        self.availables = list(range(self.width * self.height))
        self.states = {}
        self.last_move = -1

    def move_to_location(self, move):
        """Move(int) --> 坐标[row, col]"""
        row = move // self.width
        col = move % self.width
        return [row, col]

    def location_to_move(self, location):
        """坐标[行,列] --> Move(int)"""
        if len(location) != 2:
            return -1
        row, col = location
        move = row * self.width + col
        if move not in range(self.width * self.height):
            return -1
        return move

    def current_state(self):
        """返回当前棋局状态的4通道表示"""
        square_state = np.zeros((4, self.width, self.height))
        if self.states:
            for move, player in self.states.items():
                # 通道0: 当前玩家棋子位置
                # 通道1: 对手棋子位置
                if player == self.current_player:
                    square_state[0][move // self.width, move % self.height] = 1.0
                else:
                    square_state[1][move // self.width, move % self.height] = 1.0
                # 通道2: 最后一步位置
                if move == self.last_move:
                    square_state[2][move // self.width, move % self.height] = 1.0
        # 通道3: 当前玩家标识 (1表示当前玩家)
        square_state[3][:, :] = 1.0 if self.current_player == 1 else 0.0
        return square_state[:, ::-1, :]  # 行翻转保持一致性

    def do_move(self, move):
        """执行落子"""
        self.states[move] = self.current_player
        self.availables.remove(move)
        self.current_player = 2 if self.current_player == 1 else 1  # 切换玩家
        self.last_move = move

    def has_a_winner(self):
        """判断是否有玩家赢得游戏"""
        width, height, n = self.width, self.height, self.n_in_row
        moved = list(set(range(width * height)) - set(self.availables))
        
        if len(moved) < n * 2 - 1:
            return False, -1  # 尚不足以产生胜负

        for m in moved:
            h = m // width
            w = m % width
            player = self.states[m]
            
            # 检查四个方向的连子
            directions = [
                (0, 1),   # 水平
                (1, 0),    # 垂直
                (1, 1),    # 主对角线
                (1, -1)    # 副对角线
            ]
            
            for dh, dw in directions:
                count = 1
                for step in range(1, n):
                    if (w + dw * step < 0 or w + dw * step >= width or
                        h + dh * step < 0 or h + dh * step >= height):
                        break
                    if self.states.get(m + dh * step * width + dw * step, -1) != player:
                        break
                    count += 1
                if count >= n:
                    return True, player

        return False, -1

    def game_end(self):
        """判断游戏是否结束"""
        win, winner = self.has_a_winner()
        if win:
            return True, winner
        elif not self.availables:
            return True, -1  # 平局
        return False, -1

    def get_current_player(self):
        """获取当前玩家ID"""
        return self.current_player


class Game:
    def __init__(self, board):
        self.board = board

    def start_evaluative_play(self, player1, player2, start_player=1):
        """
        增强版对局方法，记录完整对局数据
        返回: (winner, moves, advantages)
        """
        self.board.init_board(start_player)
        advantages = []
        
        while True:
            current_player = player1 if self.board.current_player == 1 else player2
            move = current_player.get_action(self.board)
            self.board.do_move(move)
            
            # 记录当前优势 (使用玩家1的视角)
            if hasattr(player1, 'policy_value_fn'):
                _, value = player1.policy_value_fn(self.board)
                advantages.append(value if self.board.current_player == 2 else -value)
            
            end, winner = self.board.game_end()
            if end:
                return winner, self.board.states, advantages
            
    def graphic(self, board, player1_id, player2_id):
        """打印棋盘状态"""
        os.system('cls' if os.name == 'nt' else 'clear')
        width = board.width
        height = board.height
        print(f"===== 五子棋 =====\n玩家 {player1_id} (X) VS 玩家 {player2_id} (O)")
        print('   +' + '---+' * width)
        for i in range(height - 1, -1, -1):
            line = f"{i+1:2d} |"  # 行号从1开始显示
            for j in range(width):
                loc = i * width + j
                p = board.states.get(loc, -1)
                if p == 1:
                    line += ' X |'
                elif p == 2:
                    line += ' O |'
                else:
                    line += '   |'
            print(line)
            print('   +' + '---+' * width)
        col_title = '   ' + ''.join(f'{i+1:3d} ' for i in range(width))  # 列号从1开始显示
        print(col_title)

    def start_play(self, player1, player2, start_player=1, is_shown=1):
        """开始对弈
        Args:
            player1: 玩家1的AI对象
            player2: 玩家2的AI对象
            start_player: 先手玩家ID (1或2)
            is_shown: 是否显示棋盘
        """
        self.board.init_board(start_player)
        player1.set_player_ind(1)
        player2.set_player_ind(2)
        players = {1: player1, 2: player2}

        if is_shown:
            self.graphic(self.board, 1, 2)

        while True:
            player_id = self.board.get_current_player()
            player = players[player_id]
            move = player.get_action(self.board)
            self.board.do_move(move)
            
            if is_shown:
                self.graphic(self.board, 1, 2)
                
            end, winner = self.board.game_end()
            if end:
                if is_shown:
                    print(f"游戏结束，胜者：玩家 {winner}" if winner != -1 else "游戏结束，平局")
                return winner

    def start_self_play(self, player, temp=1e-3, is_shown=0):
        """自我对弈，返回赢家及数据"""
        self.board.init_board(1)  # 默认玩家1先手
        states, mcts_probs, current_players = [], [], []

        while True:
            move, move_probs = player.get_action(self.board, temp=temp, return_prob=1)
            states.append(self.board.current_state())
            mcts_probs.append(move_probs)
            current_players.append(self.board.current_player)
            self.board.do_move(move)

            if is_shown:
                self.graphic(self.board, 1, 2)

            end, winner = self.board.game_end()
            if end:
                winners_z = np.zeros(len(current_players))
                if winner != -1:
                    winners_z[np.array(current_players) == winner] = 1.0
                    winners_z[np.array(current_players) != winner] = -1.0

                player.reset_player()
                if is_shown:
                    print(f"游戏结束，赢家：玩家 {winner}" if winner != -1 else "游戏结束，平局")

                return winner, zip(states, mcts_probs, winners_z)


│  human_play.py
# -*- coding: utf-8 -*-
"""
人机对战脚本，使用训练好的PyTorch模型
交互式命令行输入坐标进行落子
"""
import torch
import pickle
from game import Board, Game
from mcts_alphaZero import MCTSPlayer
from policy_value_net_pytorch import PolicyValueNet
import argparse
import os

class Human:
    def __init__(self):
        self.player = None
    def set_player_ind(self, p):
        self.player = p
    
    def get_action(self, board):
        while True:
            try:
                location = input(f"玩家 {self.player} 回合，请输入落子位置(row,col): ")
                if isinstance(location, str):
                    location = [int(n, 10)-1 for n in location.strip().split(",")]
                move = board.location_to_move(location)
                if move in board.availables:
                    return move
                else:
                    print("无效落子，该位置已占用或超出边界，请重新输入")
            except (ValueError, IndexError):
                print("输入格式无效，请输入格式 'row,col' ，例如 '2,3'")
    
    def __str__(self):
        return f"Human player {self.player}"

def run():
    parser = argparse.ArgumentParser(description='AlphaZero五子棋人机对战')
    
    # 使用位置参数而不是选项参数
    parser.add_argument('width', type=int, nargs='?', default=8,
                        help='棋盘宽度（默认8）')
    parser.add_argument('height', type=int, nargs='?', default=8,
                        help='棋盘高度（默认8）')
    parser.add_argument('n_in_row', type=int, nargs='?', default=5,
                        help='连子数（默认5）')
    parser.add_argument('-m', '--model', type=str, default=None,
                        help='可选：指定模型文件路径，默认自动加载 best_policy_{width}_{height}_{n_in_row}.model')
    
    args = parser.parse_args()
    width = args.width
    height = args.height
    n_in_row = args.n_in_row
    if args.model:
        model_file = args.model
    else:
        model_file = f"best_policy_{width}_{height}_{n_in_row}.model"
        if not os.path.exists(model_file):
            print(f"找不到默认模型文件：{model_file}，请确认模型是否存在或使用 -m 参数指定模型路径")
            return
    print(f"使用棋盘尺寸: {width}x{height}, 连子数: {n_in_row}")
    print(f"加载模型: {model_file}")
    
    board = Board(width=width, height=height, n_in_row=n_in_row)
    game = Game(board)
    try:
        best_policy = PolicyValueNet(width, height, model_file=model_file, use_gpu=True)
    except RuntimeError as e:
        print(f"加载模型失败，可能棋盘尺寸或连子数与训练模型不匹配: {e}")
        return
    
    mcts_player = MCTSPlayer(best_policy.policy_value_fn, c_puct=5, n_playout=400)
    human = Human()  # 使用当前文件中定义的Human类
    try:
        game.start_play(human, mcts_player, start_player=1, is_shown=1)
    except KeyboardInterrupt:
        print("游戏中断，退出")

if __name__ == '__main__':
    run()


│  log_analysis.ipynb

│  mcts_alphaZero.py
# -*- coding: utf-8 -*-
"""
AlphaZero风格的蒙特卡洛树搜索实现，结合神经网络策略价值网络引导搜索

作者：Junxiao Song，中文注释整理
"""

import numpy as np
import copy
import math

def softmax(x):
    probs = np.exp(x - np.max(x))
    probs /= np.sum(probs)
    return probs


class TreeNode:
    """蒙特卡洛树搜索节点"""

    def __init__(self, parent, prior_p):
        self._parent = parent
        self._children = {}  # action -> TreeNode
        self._n_visits = 0
        self._Q = 0           # 节点的价值估计
        self._u = 0           # 置信上界按摩
        self._P = prior_p     # 先验概率

    def expand(self, action_priors):
        """扩展子节点，action_priors是(action, prob)列表"""
        for action, prob in action_priors:
            if action not in self._children:
                self._children[action] = TreeNode(self, prob)

    def select(self, c_puct):
        """选择子节点，返回(动作, 下一个节点)"""
        return max(self._children.items(),
                   key=lambda act_node: act_node[1].get_value(c_puct))

    def update(self, leaf_value):
        """根据叶节点价值更新节点信息"""
        self._n_visits += 1
        self._Q += (leaf_value - self._Q) / self._n_visits

    def update_recursive(self, leaf_value):
        """递归更新祖先节点"""
        if self._parent:
            self._parent.update_recursive(-leaf_value)
        self.update(leaf_value)

    def get_value(self, c_puct):
        """计算节点评分， Q + U """
        self._u = (c_puct * self._P *
                   math.sqrt(self._parent._n_visits) / (1 + self._n_visits))
        return self._Q + self._u

    def is_leaf(self):
        return len(self._children) == 0

    def is_root(self):
        return self._parent is None


class MCTS:
    """蒙特卡洛树搜索主类"""

    def __init__(self, policy_value_fn, c_puct=5, n_playout=400):
        """
        policy_value_fn: 输入棋盘状态，输出(action, prob)元组列表及估值[-1,1]
        c_puct: 调节探索权重
        n_playout: 每步模拟次数
        """
        self._root = TreeNode(None, 1.0)
        self._policy = policy_value_fn
        self._c_puct = c_puct
        self._n_playout = n_playout

    def _playout(self, state):
        """从根节点到叶节点进行一次模拟"""
        node = self._root
        while True:
            if node.is_leaf():
                break
            action, node = node.select(self._c_puct)
            state.do_move(action)

        action_probs, leaf_value = self._policy(state)

        end, winner = state.game_end()
        if not end:
            node.expand(action_probs)
        else:
            if winner == -1:
                leaf_value = 0.0
            else:
                leaf_value = 1.0 if winner == state.get_current_player() else -1.0

        # 反向传播价值
        node.update_recursive(-leaf_value)

    def get_move_probs(self, state, temp=1e-3):
        """运行所有模拟，输出动作及概率"""
        for _ in range(self._n_playout):
            state_copy = copy.deepcopy(state)
            self._playout(state_copy)

        act_visits = [(act, node._n_visits) for act, node in self._root._children.items()]
        if not act_visits:
            return [], []

        acts, visits = zip(*act_visits)
        act_probs = softmax(1.0 / temp * np.log(np.array(visits) + 1e-10))
        return acts, act_probs

    def update_with_move(self, last_move):
        """移动后更新根节点"""
        if last_move in self._root._children:
            self._root = self._root._children[last_move]
            self._root._parent = None
        else:
            self._root = TreeNode(None, 1.0)

    def __str__(self):
        return "MCTS"


class MCTSPlayer:
    """基于MCTS的AI玩家"""

    def __init__(self, policy_value_function,
                 c_puct=5, n_playout=400, is_selfplay=False):
        self.mcts = MCTS(policy_value_function, c_puct, n_playout)
        self._is_selfplay = is_selfplay

    def set_player_ind(self, p):
        self.player = p

    def reset_player(self):
        self.mcts.update_with_move(-1)

    def get_action(self, board, temp=1e-3, return_prob=False):
        sensible_moves = board.availables
        move_probs = np.zeros(board.width * board.height)
        if len(sensible_moves) > 0:
            acts, probs = self.mcts.get_move_probs(board, temp)
            move_probs[list(acts)] = probs

            if self._is_selfplay:
                # 加入Dirichlet噪声，增强探索
                move = np.random.choice(
                    acts,
                    p=0.75 * probs + 0.25 * np.random.dirichlet(0.3 * np.ones(len(probs)))
                )
                self.mcts.update_with_move(move)
            else:
                move = np.random.choice(acts, p=probs)
                self.mcts.update_with_move(-1)

            if return_prob:
                return move, move_probs
            else:
                return move
        else:
            print("WARNING: the board is full")
            return -1

    def __str__(self):
        return f"MCTS Player {self.player}"


│  mcts_pure.py
# -*- coding: utf-8 -*-
"""
纯蒙特卡洛树搜索实现（无神经网络）
作者：Junxiao Song，中文注释整理
"""

import numpy as np
import copy
from operator import itemgetter
from config import config  # 添加在文件顶部

def rollout_policy_fn(board):
    """随机模拟策略函数，用于快速蒙特卡洛模拟"""
    action_probs = np.random.rand(len(board.availables))
    return zip(board.availables, action_probs)


def policy_value_fn(board):
    """纯MCTS使用均匀概率且不评估当前局面价值"""
    action_probs = np.ones(len(board.availables)) / len(board.availables)
    return zip(board.availables, action_probs), 0


class TreeNode:
    """MCTS中的搜索树节点"""
    def __init__(self, parent, prior_p):
        self._parent = parent
        self._children = {}  # key:action, value:TreeNode
        self._n_visits = 0
        self._Q = 0
        self._u = 0
        self._P = prior_p

    def expand(self, action_priors):
        """扩展子节点"""
        for action, prob in action_priors:
            if action not in self._children:
                self._children[action] = TreeNode(self, prob)

    def select(self, c_puct):
        """选择最佳子节点，根据Q+U选择"""
        return max(self._children.items(), key=lambda act_node: act_node[1].get_value(c_puct))

    def update(self, leaf_value):
        """根据叶节点的估值更新节点"""
        self._n_visits += 1
        self._Q += 1.0 * (leaf_value - self._Q) / self._n_visits

    def update_recursive(self, leaf_value):
        """递归更新父节点"""
        if self._parent:
            self._parent.update_recursive(-leaf_value)
        self.update(leaf_value)

    def get_value(self, c_puct):
        """计算节点价值"""
        self._u = (c_puct * self._P * np.sqrt(self._parent._n_visits) / (1 + self._n_visits))
        return self._Q + self._u

    def is_leaf(self):
        return len(self._children) == 0

    def is_root(self):
        return self._parent is None


class MCTS:
    """纯MCTS算法"""

    def __init__(self, policy_value_fn, c_puct=5, n_playout=1000):
        self._root = TreeNode(None, 1.0)
        self._policy = policy_value_fn
        self._c_puct = c_puct
        self._n_playout = n_playout

    def _playout(self, state):
        node = self._root
        while True:
            if node.is_leaf():
                break
            action, node = node.select(self._c_puct)
            state.do_move(action)

        action_probs, _ = self._policy(state)
        end, winner = state.game_end()
        if not end:
            node.expand(action_probs)

        leaf_value = self._evaluate_rollout(state)

        node.update_recursive(-leaf_value)

    def _evaluate_rollout(self, state, limit=1000):
        player = state.get_current_player()
        for i in range(limit):
            end, winner = state.game_end()
            if end:
                break
            action_probs = rollout_policy_fn(state)
            max_action = max(action_probs, key=itemgetter(1))[0]
            state.do_move(max_action)
        if winner == -1:
            return 0
        else:
            return 1 if winner == player else -1

    def get_move(self, state):
        for n in range(self._n_playout):
            state_copy = copy.deepcopy(state)
            self._playout(state_copy)
        return max(self._root._children.items(), key=lambda act_node: act_node[1]._n_visits)[0]

    def update_with_move(self, last_move):
        if last_move in self._root._children:
            self._root = self._root._children[last_move]
            self._root._parent = None
        else:
            self._root = TreeNode(None, 1.0)

    def __str__(self):
        return "Pure MCTS"




class MCTSPlayer:
    def __init__(self, c_puct=5, n_playout=None):  # 修改参数为可选
        """
        修改后的初始化方法：
        - 优先使用传入的n_playout参数
        - 未传入时自动使用config.pure_mcts_playout_num
        """
        self.mcts = MCTS(policy_value_fn, 
                        c_puct=c_puct,
                        n_playout=n_playout if n_playout is not None else config.pure_mcts_playout_num)
        self.player = None

    def set_player_ind(self, p):
        self.player = p

    def reset_player(self):
        self.mcts.update_with_move(-1)

    def get_action(self, board):
        sensible_moves = board.availables
        if len(sensible_moves) > 0:
            move = self.mcts.get_move(board)
            self.mcts.update_with_move(-1)
            return move
        else:
            print("WARNING: the board is full")
            return -1

    def __str__(self):
        return f"Pure MCTS Player {self.player}"


│  minimax.py
# -*- coding: utf-8 -*-
"""
Minimax算法实现
"""

import numpy as np
from game import Board

class MinimaxPlayer:
    def __init__(self, depth=3):
        self.depth = depth
        self.board_width = 8  # 从config获取
        self.board_height = 8
        
    def get_action(self, board):
        """获取最佳移动"""
        best_move = None
        best_value = -np.inf
        
        for move in board.availables:
            board.do_move(move)
            value = self.minimax(board, self.depth, False)
            board.undo_move(move)
            
            if value > best_value:
                best_value = value
                best_move = move
                
        return best_move
    
    def minimax(self, board, depth, is_maximizing):
        """Minimax算法核心"""
        if depth == 0 or board.game_end():
            return self.evaluate(board)
            
        if is_maximizing:
            value = -np.inf
            for move in board.availables:
                board.do_move(move)
                value = max(value, self.minimax(board, depth-1, False))
                board.undo_move(move)
            return value
        else:
            value = np.inf
            for move in board.availables:
                board.do_move(move)
                value = min(value, self.minimax(board, depth-1, True))
                board.undo_move(move)
            return value
    
    def evaluate(self, board):
        """简单评估函数"""
        # 基础实现 - 可扩展
        if board.current_player == 1:
            return len(board.availables)
        return -len(board.availables)

class MinimaxABPlayer(MinimaxPlayer):
    """带Alpha-Beta剪枝的Minimax"""
    
    def get_action(self, board):
        best_move = None
        alpha = -np.inf
        beta = np.inf
        
        for move in board.availables:
            board.do_move(move)
            value = self.minimax_ab(board, self.depth, alpha, beta, False)
            board.undo_move(move)
            
            if value > alpha:
                alpha = value
                best_move = move
                
        return best_move
    
    def minimax_ab(self, board, depth, alpha, beta, is_maximizing):
        if depth == 0 or board.game_end():
            return self.evaluate(board)
            
        if is_maximizing:
            value = -np.inf
            for move in board.availables:
                board.do_move(move)
                value = max(value, self.minimax_ab(board, depth-1, alpha, beta, False))
                board.undo_move(move)
                alpha = max(alpha, value)
                if alpha >= beta:
                    break
            return value
        else:
            value = np.inf
            for move in board.availables:
                board.do_move(move)
                value = min(value, self.minimax_ab(board, depth-1, alpha, beta, True))
                board.undo_move(move)
                beta = min(beta, value)
                if alpha >= beta:
                    break
            return value


│  policy_value_net_pytorch.py
# -*- coding: utf-8 -*-
"""
AlphaZero策略价值网络 - PyTorch实现
支持GPU加速，基于CNN
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np



def set_learning_rate(optimizer, lr):
    """调整优化器学习率"""
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr


class Net(nn.Module):
    """策略价值网络结构"""
    def __init__(self, board_width, board_height):
        super(Net, self).__init__()
        self.board_width = board_width
        self.board_height = board_height

        # 共有卷积层
        self.conv1 = nn.Conv2d(4, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)

        # 策略头
        self.act_conv1 = nn.Conv2d(128, 4, kernel_size=1)
        self.act_fc1 = nn.Linear(4 * board_width * board_height,
                                 board_width * board_height)

        # 价值头
        self.val_conv1 = nn.Conv2d(128, 2, kernel_size=1)
        self.val_fc1 = nn.Linear(2 * board_width * board_height, 64)
        self.val_fc2 = nn.Linear(64, 1)

    def forward(self, state_input):
        # 共享卷积
        x = F.relu(self.conv1(state_input))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        # 策略头前向
        x_act = F.relu(self.act_conv1(x))
        x_act = x_act.view(-1, 4 * self.board_width * self.board_height)
        # 修正log_softmax警告，显式指定dim=1
        x_act = F.log_softmax(self.act_fc1(x_act), dim=1)

        # 价值头前向
        x_val = F.relu(self.val_conv1(x))
        x_val = x_val.view(-1, 2 * self.board_width * self.board_height)
        x_val = F.relu(self.val_fc1(x_val))
        x_val = torch.tanh(self.val_fc2(x_val))

        return x_act, x_val


class PolicyValueNet:
    def load_model(self, model_file):
        """安全加载模型，包含完整验证逻辑"""
        try:
            # 加载检查点（自动处理设备位置）
            checkpoint = torch.load(model_file, map_location=self.device)
            
            # 验证模型格式
            if isinstance(checkpoint, dict):
                # 新版模型格式验证
                if 'state_dict' not in checkpoint:
                    raise ValueError("模型文件缺少state_dict字段")
                
                # 棋盘尺寸验证
                saved_w = checkpoint.get('board_width')
                saved_h = checkpoint.get('board_height')
                if (saved_w is not None and saved_h is not None) and \
                (saved_w != self.board_width or saved_h != self.board_height):
                    raise ValueError(
                        f"模型棋盘大小({saved_w}x{saved_h})与当前({self.board_width}x{self.board_height})不匹配"
                    )
                
                # 加载状态字典
                state_dict = checkpoint['state_dict']
            else:
                # 旧版模型兼容
                state_dict = checkpoint
                print("警告：加载旧版模型格式，建议重新保存为新格式")
            
            # 网络结构兼容性检查
            current_state_dict = self.policy_value_net.state_dict()
            matched_state_dict = {}
            
            for k, v in state_dict.items():
                if k in current_state_dict:
                    if v.shape == current_state_dict[k].shape:
                        matched_state_dict[k] = v
                    else:
                        print(f"警告：跳过参数 {k} (形状不匹配: {v.shape} vs {current_state_dict[k].shape})")
                else:
                    print(f"警告：跳过未使用参数 {k}")
            
            # 部分加载
            self.policy_value_net.load_state_dict(matched_state_dict, strict=False)
            
        except FileNotFoundError:
            raise FileNotFoundError(f"模型文件不存在: {model_file}")
        except Exception as e:
            raise RuntimeError(f"加载模型失败: {str(e)}")
    
    
    def __init__(self, board_width, board_height, model_file=None, use_gpu=False):
        self.use_gpu = use_gpu
        self.board_width = board_width
        self.board_height = board_height
        self.l2_const = 1e-4
        # 设备设置
        self.device = torch.device('cuda' if self.use_gpu and torch.cuda.is_available() else 'cpu')
        
        # CUDA优化配置
        if self.device.type == 'cuda':
            torch.backends.cudnn.benchmark = True  # 自动寻找最优卷积算法
            torch.backends.cudnn.deterministic = False  # 允许非确定性算法以获得更好性能
        
        # 初始化网络
        self.policy_value_net = Net(board_width, board_height).to(self.device)
        
        # PyTorch 2.0+模型编译
        #if hasattr(torch, 'compile') and use_gpu and torch.cuda.is_available():
        #    try:
        #        import triton  # 检查是否安装
        #        self.policy_value_net = torch.compile(
        #            self.policy_value_net,
        #            mode='default',  # 改为default更稳定
        #            fullgraph=False,
        #            dynamic=False
        #        )
        #        print("模型编译优化已启用")
        #    except Exception as e:
         #       print(f"模型编译失败，使用原始模式: {str(e)}")
        
        # 优化器设置
        self.optimizer = optim.Adam(
            self.policy_value_net.parameters(), 
            weight_decay=self.l2_const
        )

        print(next(self.policy_value_net.parameters()).device)  # 确认设备
        print(torch.backends.cudnn.benchmark)  # 确认优化开启

        # 模型加载
        if model_file:
            self.load_model(model_file)
        self.writer = None  # 初始化writer引用
        self.train_step_count = 0  # 训练步数计数器
    
    def set_writer(self, writer):
        """接收外部的TensorBoard writer"""
        self.writer = writer
    
    def _convert_to_tensor(self, data, dtype=np.float32):
        """统一的张量转换方法"""
        if isinstance(data, list):
            data = np.ascontiguousarray(data, dtype=dtype)
        if self.use_gpu:
            return torch.from_numpy(data).float().cuda()
        return torch.from_numpy(data).float()
    
    def policy_value(self, state_batch):
        """批量状态输入，输出动作概率和状态值"""
        state_batch = self._convert_to_tensor(state_batch)
        self.policy_value_net.eval()
        with torch.no_grad():
            log_act_probs, value = self.policy_value_net(state_batch)
            act_probs = torch.exp(log_act_probs).cpu().numpy()
            value = value.cpu().numpy()
        return act_probs, value

    def policy_value_fn(self, board):
        """
        输入单个棋盘状态，输出合法动作和对应概率，以及状态价值评估
        """
        legal_positions = board.availables
        current_state = np.ascontiguousarray(board.current_state().reshape(
            -1, 4, self.board_width, self.board_height))

        if self.use_gpu:
            import torch
            from torch.autograd import Variable
            state_tensor = torch.from_numpy(current_state).cuda().float()
            self.policy_value_net.eval()
            with torch.no_grad():
                log_act_probs, value = self.policy_value_net(state_tensor)
                act_probs = torch.exp(log_act_probs).cpu().numpy().flatten()
                value = value.cpu().numpy()[0][0]
        else:
            from torch.autograd import Variable
            state_tensor = torch.from_numpy(current_state).float()
            self.policy_value_net.eval()
            with torch.no_grad():
                log_act_probs, value = self.policy_value_net(state_tensor)
                act_probs = torch.exp(log_act_probs).numpy().flatten()
                value = value.numpy()[0][0]

        act_probs = [(pos, act_probs[pos]) for pos in legal_positions]
        return act_probs, value

    def train_step(self, state_batch, mcts_probs, winner_batch, lr):
        """执行一次训练步骤，返回 loss 和 entropy"""
        # 统一使用优化后的转换方法
        state_batch = self._convert_to_tensor(state_batch)
        mcts_probs = self._convert_to_tensor(mcts_probs)
        winner_batch = self._convert_to_tensor(winner_batch)

        self.policy_value_net.train()
        self.optimizer.zero_grad()
        set_learning_rate(self.optimizer, lr)

        log_act_probs, value = self.policy_value_net(state_batch)

        value_loss = F.mse_loss(value.view(-1), winner_batch)
        policy_loss = -torch.mean(torch.sum(mcts_probs * log_act_probs, 1))
        loss = value_loss + policy_loss

        loss.backward()
        self.optimizer.step()

        entropy = -torch.mean(torch.sum(torch.exp(log_act_probs) * log_act_probs, 1))
        # PyTorch 0.5+的版本用 .item() 获取标量值
        
    
        return loss.item(), entropy.item()

    def get_policy_param(self):
        return self.policy_value_net.state_dict()


    # 修改后
    def save_model(self, model_file, board_width=None, board_height=None, n_in_row=None):
        """保存模型，并在文件名中包含棋盘尺寸和连子数量信息"""
        # 获取原始文件名（不包含扩展名）
        if board_width is None or board_height is None or n_in_row is None:
            board_width = self.board_width
            board_height = self.board_height
        if '.' in model_file:
            base_name = model_file.rsplit('.', 1)[0]
            ext = '.' + model_file.rsplit('.', 1)[1]
        else:
            base_name = model_file
            ext = '.model'
        # 构建新文件名
        new_model_file = f"{base_name}_{board_width}_{board_height}_{n_in_row}{ext}"
        
        # 保存模型
        torch.save({
            'state_dict': self.get_policy_param(),
            'board_width': board_width,
            'board_height': board_height,
            'n_in_row': n_in_row
        }, new_model_file)
        
        return new_model_file


│  train.py
# -*- coding: utf-8 -*-
"""
AlphaZero五子棋训练脚本 - 增强评估版
"""

import random
import numpy as np
from collections import deque, defaultdict
from game import Board, Game
from mcts_alphaZero import MCTSPlayer
from policy_value_net_pytorch import PolicyValueNet
from config import config
from config import Config
from utils import init_logger, ProgressBar
from evaluation_metrics import EvaluationMetrics
import logging
import torch
import time
from datetime import datetime

class TrainPipeline:
    def __init__(self, config):
        # 初始化日志
        self.config = config
        self.logger = init_logger(config.log_file)
        self.logger.info("初始化训练流水线")
        
        # 初始化棋盘和游戏
        self.board = Board(config.board_width,
                         config.board_height,
                         config.n_in_row)
        self.game = Game(self.board)

        # 初始化策略网络
        self.policy_value_net = PolicyValueNet(config.board_width,
                                            config.board_height,
                                            use_gpu=config.use_gpu)

        # 蒙特卡洛树搜索玩家
        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,
                                    c_puct=config.c_puct,
                                    n_playout=config.n_playout,
                                    is_selfplay=True)

        # 训练数据缓冲区
        self.data_buffer = deque(maxlen=config.buffer_size)

        # 超参数
        self.learn_rate = config.lr
        self.lr_multiplier = 1.0
        self.temp = config.temp
        self.batch_size = config.batch_size
        self.epochs = config.epochs
        self.play_batch_size = config.play_batch_size
        self.check_freq = config.check_freq
        self.game_batch_num = config.game_batch_num
        self.best_win_ratio = 0.0
        self.pure_mcts_playout_num = config.pure_mcts_playout_num
        self.episode_len = 0
        self.last_loss = 0
        self.last_win_ratio = 0
        
        # 评估系统增强
        self.evaluators = {
            'pure_mcts': EvaluationMetrics(),
            'minimax': EvaluationMetrics(),
            'minimax_ab': EvaluationMetrics()
        }

    
    def get_equi_data(self, play_data):
        """增强数据集（带内存安全改进）"""
        extend_data = []
        temp_objects = {
            'equi_state': None,
            'equi_mcts_prob': None,
            'flipped_state': None,
            'flipped_prob': None
        }
        
        try:
            for state, mcts_prob, winner in play_data:
                extend_data.append((state.copy(), mcts_prob.copy(), winner))
                
                for i in [1, 2, 3, 4]:
                    temp_objects['equi_state'] = np.array([np.rot90(s, i) for s in state])
                    prob_matrix = mcts_prob.reshape(config.board_height, config.board_width)
                    temp_objects['equi_mcts_prob'] = np.rot90(np.flipud(prob_matrix), i)
                    
                    extend_data.append((
                        temp_objects['equi_state'].copy(),
                        np.flipud(temp_objects['equi_mcts_prob']).flatten().copy(),
                        winner
                    ))
                    
                    temp_objects['flipped_state'] = np.array([np.fliplr(s) for s in temp_objects['equi_state']])
                    temp_objects['flipped_prob'] = np.fliplr(temp_objects['equi_mcts_prob'])
                    
                    extend_data.append((
                        temp_objects['flipped_state'].copy(),
                        np.flipud(temp_objects['flipped_prob']).flatten().copy(),
                        winner
                    ))
                    
            return extend_data
        finally:
            keys_to_delete = [k for k in temp_objects if temp_objects[k] is not None]
            for key in keys_to_delete:
                if temp_objects[key] is not None:
                    del temp_objects[key]

    def collect_selfplay_data(self):
        """收集自对弈数据"""
        for _ in range(self.config.play_batch_size):
            winner, play_data = self.game.start_self_play(self.mcts_player,
                                                        temp=self.config.temp)
            play_data = list(play_data)[:]
            self.episode_len = len(play_data)
            play_data = self.get_equi_data(play_data)
            self.data_buffer.extend(play_data)

    def policy_update(self):
        """训练策略网络一次"""
        mini_batch = random.sample(self.data_buffer, self.batch_size)
        state_batch = [data[0] for data in mini_batch]
        mcts_probs_batch = [data[1] for data in mini_batch]
        winner_batch = [data[2] for data in mini_batch]

        old_probs, old_v = self.policy_value_net.policy_value(state_batch)

        for i in range(self.epochs):
            loss, entropy = self.policy_value_net.train_step(
                state_batch,
                mcts_probs_batch,
                winner_batch,
                self.learn_rate * self.lr_multiplier
            )
            new_probs, new_v = self.policy_value_net.policy_value(state_batch)

            kl = np.mean(np.sum(old_probs * (
                np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)), axis=1))

            if kl > self.kl_targ * 4:
                break

        # 学习率自适应调整
        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:
            self.lr_multiplier /= 1.5
        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:
            self.lr_multiplier *= 1.5

        explained_var_old = 1 - np.var(np.array(winner_batch) - old_v.flatten()) / np.var(np.array(winner_batch))
        explained_var_new = 1 - np.var(np.array(winner_batch) - new_v.flatten()) / np.var(np.array(winner_batch))

        self.logger.info(
            f"kl:{kl:.5f}, lr_multiplier:{self.lr_multiplier:.3f}, loss:{loss:.4f}, entropy:{entropy:.4f}, "
            f"explained_var_old:{explained_var_old:.3f}, explained_var_new:{explained_var_new:.3f}"
        )
        
        self.last_loss = loss
        return loss, entropy

    def _get_opponent_config(self, opponent_type):
        """获取对手配置信息"""
        if opponent_type == 'pure_mcts':
            return f"n_playout={config.pure_mcts_playout_num} (纯MCTS模拟次数)"
        elif opponent_type == 'minimax':
            return f"depth=3 (Minimax搜索深度)"
        elif opponent_type == 'minimax_ab':
            return f"depth=3 (Minimax+AB剪枝)"
        else:
            return "自定义对手"
        
    def evaluate_against(self, opponent_type, eval_games):
        """
        通用评估方法
        :param opponent_type: pure_mcts/minimax/minimax_ab
        :param eval_games: 评估局数
        """
        from mcts_pure import MCTSPlayer as MCTS_Pure
        from minimax import MinimaxPlayer
        from minimax import MinimaxABPlayer
        
        # 初始化胜率统计字典（修复点1）
        win_cnt = {1: 0, 2: 0, -1: 0}  # 明确初始化所有可能结果
        
        # 初始化对手
        if opponent_type == 'pure_mcts':
            opponent = MCTS_Pure(c_puct=config.c_puct,
                                n_playout=config.pure_mcts_playout_num)
        elif opponent_type == 'minimax':
            opponent = MinimaxPlayer(depth=3)
        elif opponent_type == 'minimax_ab':
            opponent = MinimaxABPlayer(depth=3)
        else:
            raise ValueError(f"未知对手类型: {opponent_type}")
        
        current_player = MCTSPlayer(self.policy_value_net.policy_value_fn,
                                c_puct=config.c_puct,
                                n_playout=config.n_playout)
        
        evaluator = self.evaluators[opponent_type]
        evaluator.reset()
        
        for i in range(eval_games):
            try:
                winner, moves, advantages = self.game.start_evaluative_play(
                    current_player if i % 2 == 0 else opponent,
                    current_player if i % 2 == 1 else opponent,
                    start_player=1 if i % 2 == 0 else 2
                )
                win_cnt[winner] += 1  # 安全计数（修复点2）
                evaluator.record_game(winner, moves, advantages)
            except Exception as e:
                self.logger.error(f"评估对局异常: {str(e)}")
                continue
        
        # 安全计算胜率（修复点3）
        total_games = sum(win_cnt.values())
        win_ratio = win_cnt[1] / total_games if total_games > 0 else 0.0
        
        self.logger.info(
            f"\n[评估 vs {opponent_type.upper()}]"
            f"\n[配置对比]"
            f"\n  主MCTS: n_playout={config.n_playout} | c_puct={config.c_puct}"
            f"\n  基准AI: {self._get_opponent_config(opponent_type)}"
            f"\n[对局结果]"
            f"\n  胜/负/平: {win_cnt[1]}/{win_cnt[2]}/{win_cnt[-1]}"
            f"\n  胜率: {win_ratio:.1%}"
        )
        
        return win_ratio

    def policy_evaluate(self):
        """综合评估策略（最小修改版）"""
        win_ratios = {}
        
        for opponent in ['pure_mcts', 'minimax', 'minimax_ab']:
            try:
                win_ratios[opponent] = self.evaluate_against(
                    opponent,
                    max(1, self.config.eval_games // 3)  # 确保至少1局
                )
            except Exception as e:
                self.logger.error(f"评估{opponent}时出错: {str(e)}")
                win_ratios[opponent] = 0.0
        
        return win_ratios.get('pure_mcts', 0.0)  # 主评估指标

    def run(self):
        """增强的训练流程"""
        self.kl_targ = self.config.kl_targ
        progress = ProgressBar(self.game_batch_num)
        
        try:
            for i in range(self.game_batch_num):
                self.collect_selfplay_data()
                self.logger.info(f"训练批次: {i + 1}, 当前对局长度: {self.episode_len}")
                
                if len(self.data_buffer) > self.batch_size:
                    loss, _ = self.policy_update()
                    progress.update(i + 1, loss=loss)
                else:
                    progress.update(i + 1)

                if (i + 1) % self.check_freq == 0:
                    win_ratio = self.policy_evaluate()
                    progress.update(i + 1, win_ratio=win_ratio)
                    
                    # 模型保存逻辑 (保持原有)
                    if win_ratio > self.best_win_ratio:
                        self.best_win_ratio = win_ratio
                        self.policy_value_net.save_model(
                            f"{self.config.model_dir}/best_policy.model"
                        )
            
            progress.end()
                
        except KeyboardInterrupt:
            print("\n训练被中断")
            self.logger.info("训练中断，退出...")

if __name__ == '__main__':
    config = Config()
    print("有效配置:", {k:v for k,v in vars(config).items() if not k.startswith('_')})
    
    logger = init_logger(config.log_file)
    logger.info("开始训练...")
    
    trainer = TrainPipeline(config)
    try:
        trainer.run()
    except KeyboardInterrupt:
        print("\n训练被用户中断")


│  utils.py
# -*- coding: utf-8 -*-
"""
工具函数模块：
- 初始化日志
- 进度条显示
"""

import logging
import time
import sys
from datetime import datetime, timedelta

def init_logger(log_file):
    """
    初始化日志，只输出到文件不显示在控制台
    """
    logger = logging.getLogger('train_logger')
    logger.setLevel(logging.INFO)
    
    # 如果已存在handlers，先清除
    if logger.handlers:
        logger.handlers = []
    
    # 文件handler，设置为追加模式
    fh = logging.FileHandler(log_file, encoding='utf-8')
    fh.setFormatter(logging.Formatter('[%(asctime)s] %(levelname)s: %(message)s'))
    logger.addHandler(fh)
    
    # 禁止传播到根logger，避免控制台输出
    logger.propagate = False
    
    return logger

class ProgressBar:
    def __init__(self, total, length=50):
        """
        :param total: 总批次数
        :param length: 进度条长度(字符数)
        """
        self.total = total
        self.length = length
        self.start_time = time.time()
        self.last_time = self.start_time
        self.time_history = []
        self.last_loss = 0
        self.last_win_ratio = 0
        
    def update(self, iteration, loss=None, win_ratio=None):
        now = time.time()
        elapsed = now - self.last_time
        self.last_time = now
        
        # 更新记录值
        if loss is not None:
            self.last_loss = loss
        if win_ratio is not None:
            self.last_win_ratio = win_ratio
        
        # 计算平均批次时间（滑动窗口）
        self.time_history.append(elapsed)
        if len(self.time_history) > 10:  # 保留最近10个批次时间
            self.time_history.pop(0)
        avg_time = sum(self.time_history) / len(self.time_history)
        
        # 进度计算
        percent = ("{0:.1f}").format(100 * (iteration / float(self.total)))
        filled_length = int(self.length * iteration // self.total)
        bar = "" * filled_length + "-" * (self.length - filled_length)
        
        # 时间计算
        elapsed_total = now - self.start_time
        remaining = (self.total - iteration) * avg_time
        
        # 打印进度条
        sys.stdout.write(
            f"\r批次 {iteration}/{self.total} |{bar}| {percent}% "
            f"[耗时: {timedelta(seconds=int(elapsed_total))}] "
            f"[{self._format_time(avg_time)}/批次] "
            f"[剩余: {timedelta(seconds=int(remaining))}] "
            f"Loss: {self.last_loss:.4f} "
            f"Win%: {self.last_win_ratio*100:.1f}%"
        )
        sys.stdout.flush()
        
    def _format_time(self, seconds):
        """格式化时间显示"""
        if seconds < 1:
            return f"{seconds*1000:.0f}ms"
        return f"{seconds:.1f}s"
    
    def end(self):
        total_time = time.time() - self.start_time
        print(f"\n? 训练完成! 总耗时: {timedelta(seconds=int(total_time))}")
        print(f"平均批次时间: {self._format_time(total_time/self.total)}")